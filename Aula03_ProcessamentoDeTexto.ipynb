{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "41a6aa6e",
      "metadata": {
        "id": "41a6aa6e"
      },
      "source": [
        "normalização de texto - remoção de ruido"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbe1f788",
      "metadata": {
        "id": "fbe1f788"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "original = \"Exemplo de texto com símbolos, pontuação e maiúsculas.\"\n",
        "texto_limpo = re.sub(r'[^A-Za-zÀ-ÿ\\s]', '',original)\n",
        "texto_normal = texto_limpo.lower()\n",
        "\n",
        "print(f'Texto original: {original}')\n",
        "print(f'\\nTexto limpo: {texto_limpo}')\n",
        "print(f'\\nTexto normal: {texto_normal}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9466578",
      "metadata": {
        "id": "f9466578"
      },
      "source": [
        "tokenização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86952dfb",
      "metadata": {
        "id": "86952dfb"
      },
      "outputs": [],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f8f3c6c",
      "metadata": {
        "id": "9f8f3c6c"
      },
      "outputs": [],
      "source": [
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "tokens = word_tokenize(texto_normalizado)\n",
        "\n",
        "print(f'Texto original: {original}')\n",
        "print(f'\\n\\nTexto limpo: {texto_limpo}')\n",
        "print(f'\\n\\nTexto normalizado: {texto_normalizado}')\n",
        "print(f'\\n\\nTokens extraidos: {tokens}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d16c52c",
      "metadata": {
        "id": "3d16c52c"
      },
      "source": [
        "tirar stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bad74c35",
      "metadata": {
        "id": "bad74c35"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stopwords_pt = set(stopwords.words('portuguese'))\n",
        "\n",
        "print(stopwords_pt)\n",
        "\n",
        "tokens_sem_stopwords = [palavra for palavra in tokens if palavra.lower() not in stopwords_pt]\n",
        "\n",
        "print(f'\\n\\nTokens extraidos: {tokens} + \\n qtd de tokens: {len(tokens)}')\n",
        "print(f'\\n\\nTokens extraidos: {tokens_sem_stopwords} + \\n qtd de tokens: {len(tokens_sem_stopwords)}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "831f737f",
      "metadata": {
        "id": "831f737f"
      },
      "source": [
        "stemming e lemalização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "603ef7b4",
      "metadata": {
        "id": "603ef7b4"
      },
      "outputs": [],
      "source": [
        "\n",
        "from nltk.stem import RSLPStemmer\n",
        "\n",
        "nltk.download('rslp')\n",
        "\n",
        "stemmer = RSLPStemmer()\n",
        "stemming = [stemmer.stem(palavra) for palavra in tokens_sem_stopwords]\n",
        "\n",
        "print(f'\\n\\nTokens extraídos: {tokens_sem_stopwords}')\n",
        "print(f'\\n\\nTokens radicais: {stemming}\\n\\n\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2692c1c3",
      "metadata": {
        "id": "2692c1c3"
      },
      "source": [
        "exemplo de pré-processamento completo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdc735c4",
      "metadata": {
        "id": "fdc735c4"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "texto = input(\"Inserir um texto, podendo ter símbolos: \")\n",
        "\n",
        "texto_limpo = re.sub(r'[^a-zA-Z]', ' ', texto)\n",
        "texto_lower = texto_limpo.lower()\n",
        "\n",
        "tokens = nltk.word_tokenize(texto_lower)\n",
        "\n",
        "stop_words = set(stopwords.words('portuguese'))\n",
        "palavras_filtradas = [palavra for palavra in tokens if palavra not in stop_words]\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "palavras_stemizadas = [stemmer.stem(palavra) for palavra in palavras_filtradas]\n",
        "\n",
        "print(palavras_stemizadas)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e14a5c9c",
      "metadata": {
        "id": "e14a5c9c"
      },
      "source": [
        "exemplo de estrutura de pré-processamento de texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3df1c879",
      "metadata": {
        "id": "3df1c879"
      },
      "outputs": [],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download pt_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b023ef8",
      "metadata": {
        "id": "8b023ef8"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('rslp')\n",
        "\n",
        "# (Trocar para 'en_core_web_sm' se for inglês)\n",
        "nlp = spacy.load(\"pt_core_news_sm\")\n",
        "\n",
        "texto = \"O PLN é essencial na ia.\"\n",
        "\n",
        "def normalizar_texto(texto):\n",
        "    texto = texto.lower()\n",
        "    texto = re.sub(r'https?://\\S+|www\\.\\S+', '', texto)\n",
        "    texto = re.sub(r'[^a-zA-Zá-úÁ-ÚçÇ ]', '', texto)\n",
        "    return texto\n",
        "\n",
        "texto_normalizado = normalizar_texto(texto)\n",
        "\n",
        "tokens = nltk.word_tokenize(texto_normalizado, language='portuguese')\n",
        "\n",
        "stopwords_pt = set(stopwords.words('portuguese'))\n",
        "tokens_sem_stopwords = [token for token in tokens if token not in stopwords_pt]\n",
        "\n",
        "stemmer = nltk.RSLPStemmer()\n",
        "tokens_stem = [stemmer.stem(token) for token in tokens_sem_stopwords]\n",
        "\n",
        "def lematizar_com_spacy(tokens):\n",
        "    doc = nlp(\" \".join(tokens))\n",
        "    return [token.lemma_ for token in doc]\n",
        "\n",
        "tokens_lematizados = lematizar_com_spacy(tokens_sem_stopwords)\n",
        "\n",
        "print(\"Texto Original:\\n\", texto)\n",
        "print(\"\\nTexto Normalizado:\\n\", texto_normalizado)\n",
        "print(\"\\nTokens:\\n\", tokens)\n",
        "print(\"\\nTokens Sem Stopwords:\\n\", tokens_sem_stopwords)\n",
        "print(\"\\nStemming:\\n\", tokens_stem)\n",
        "print(\"\\nLematização:\\n\", tokens_lematizados)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b624c870",
      "metadata": {
        "id": "b624c870"
      },
      "source": [
        "exemplo de modelo pré-treinado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "861a809c",
      "metadata": {
        "id": "861a809c"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"pt_core_news_sm\")\n",
        "\n",
        "textoRecebido = input(\"Texto para análise: \")\n",
        "doc = nlp(textoRecebido)\n",
        "\n",
        "print('\\nAnálise gramatical:')\n",
        "for token in doc:\n",
        "    print(f\"Palavra: {token.text}, Classe: {token.pos_}\")\n",
        "\n",
        "print(\"\\nAnalise dependências:\")\n",
        "for token in doc:\n",
        "  print(f\"Palavra: {token.text}, Depende de: {token.head.text}\")\n",
        "\n",
        "from spacy import displacy\n",
        "displacy.render(doc, style=\"dep\", jupyter=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}